ğŸ“˜ GreenTech Logistics â€“ Real-Time Data Pipeline
Kafka â†’ Airflow â†’ Glue (Great Expectations) â†’ S3 (Delta) â†’ Snowpipe â†’ Snowflake
ğŸš€ Project Overview

This project implements a real-time logistics data pipeline that ingests truck telemetry events from a Kafka producer, orchestrates processing using Airflow, validates data using Great Expectations inside AWS Glue, stores curated/rejected data in Delta format on S3, and finally loads the validated data into Snowflake tables through Snowpipe.

This pipeline ensures real-time, validated, and scalable data processing for logistics operations.

ğŸ—ï¸ High-Level Architecture

Kafka Producer (EC2)

Generates truck GPS + sensor events.

Produces messages to Kafka topic logistics-topic.

Kafka Consumer API (EC2)

Exposes API /start-consumer.

Sends success response to Airflow once data is received.

Apache Airflow (EC2)

Calls consumer API â†’ waits for data â†’ triggers Glue job.

Prevents unnecessary Glue executions.

Implements dynamic orchestration (event-driven logic).

AWS Glue Job

Reads raw JSON from S3.

Performs Data Quality using Great Expectations.

Writes curated + rejected data in Delta format:

s3://bucket/curated/

s3://bucket/rejected/

Snowflake Integration via Snowpipe

Auto-ingests both curated and rejected data into:

LOGISTICS_CURATED_TABLE

LOGISTICS_REJECTED_TABLE

ğŸ“‚ Project Folder Structure
GreenTech-Logistics/
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ kafka_to_glue_dag.py
â”‚
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer.py
â”‚   â””â”€â”€ consumer_api.py
â”‚
â”œâ”€â”€ glue/
â”‚   â””â”€â”€ glue_job.py
â”‚
â”œâ”€â”€ snowflake/
â”‚   â”œâ”€â”€ storage_integration.sql
â”‚   â”œâ”€â”€ stage_curated.sql
â”‚   â”œâ”€â”€ stage_rejected.sql
â”‚   â”œâ”€â”€ pipe_curated.sql
â”‚   â”œâ”€â”€ pipe_rejected.sql
â”‚
â”œâ”€â”€ s3/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ curated/
â”‚   â””â”€â”€ rejected/
â”‚
â”œâ”€â”€ architecture_diagram.png
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

ğŸ”§ Technologies Used
Layer	Tech
Real-time ingestion	Kafka (EC2)
API Trigger	Python + Flask (EC2)
Orchestration	Apache Airflow
ETL	AWS Glue + PySpark
Data Quality	Great Expectations
Data Format	Delta Lake
Storage	Amazon S3
Warehouse	Snowflake + Snowpipe
ğŸ§ª Data Quality Rules (Great Expectations)

These are the exact rules implemented in Glue:

1. NOT NULL checks

truck_id

timestamp

latitude

longitude

2. Type Validation

latitude & longitude â†’ must be float

speed â†’ must be float or integer

3. Value Range Validation

latitude between â€“90 to 90

longitude between â€“180 to 180

speed â‰¥ 0

4. JSON Structure Validation

Fields must match schema:

truck_id, location.latitude, location.longitude, speed, timestamp


Records failing these rules â†’ rejected folder.

ğŸª„ How Airflow Dynamic Orchestration Works

DAG calls consumer API:
http://ec2-public-ip/start-consumer

Consumer listens to Kafka.

Once consumer receives 1 message, it responds back to Airflow.

Airflow waits 5 min for more data.

After waiting, Airflow triggers Glue job.

This avoids schedule-based waste and makes pipeline fully event-driven.

ğŸ§° How Glue Job Works

Read raw JSON from S3.

Apply Great Expectations checks.

Split into:

curated_df â†’ valid

rejected_df â†’ invalid

Write both to S3 in Delta format.

Snowpipe automatically picks them up.

â„ï¸ Snowflake Setup

Inside snowflake/ folder:

Storage integration

External stages

File formats

Snowpipes for:

curated

rejected

Snowpipe continuously loads new Delta files â†’ Snowflake tables.

â–¶ï¸ How to Run
1. Start Kafka Producer
python kafka/producer.py

2. Start Kafka Consumer API
python kafka/consumer_api.py

3. Trigger Airflow DAG

Airflow UI â†’ Trigger DAG â†’ waits for data â†’ runs Glue job.

4. Check S3 Outputs
s3://bucket/curated/
s3://bucket/rejected/

5. Check Snowflake Tables
SELECT * FROM LOGISTICS_CURATED_TABLE;
SELECT * FROM LOGISTICS_REJECTED_TABLE;

ğŸ“¸ Diagram

Refer architecture_diagram.png

ğŸ‘¤ Author

K Syed Khalid Hameed