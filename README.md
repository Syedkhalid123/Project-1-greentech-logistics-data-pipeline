# Project-1-greentech-logistics-data-pipeline
Kafka â†’ Airflow â†’ Glue â†’ S3 â†’ Delta Lake â†’ Snowflake (Event Driven Pipeline)

ğŸš€ Project Overview

This project implements a real-time logistics data pipeline that:

Ingests truck telemetry events from a Kafka producer.

Orchestrates processing using Apache Airflow.

Validates data using Great Expectations inside AWS Glue.

Stores curated/rejected data in Delta format on S3.

Loads validated data into Snowflake tables through Snowpipe.

This pipeline ensures real-time, validated, and scalable data processing for logistics operations.

ğŸ—ï¸ High-Level Architecture
Kafka Producer (EC2)

Generates truck GPS + sensor events.

Produces messages to Kafka topic logistics-topic.

Kafka Consumer API (EC2)

Exposes API /start-consumer.

Sends success response to Airflow once data is received.

Apache Airflow (EC2)

Calls consumer API â†’ waits for data â†’ triggers Glue job.

Prevents unnecessary Glue executions.

Implements dynamic orchestration (event-driven logic).

AWS Glue Job

Reads raw JSON from S3.

Performs data quality checks using Great Expectations.

Writes curated + rejected data in Delta format:

s3://bucket/curated/
s3://bucket/rejected/
Snowflake Integration via Snowpipe

Auto-ingests both curated and rejected data into:

LOGISTICS_CURATED_TABLE
LOGISTICS_REJECTED_TABLE
ğŸ“‚ Project Folder Structure
GreenTech-Logistics/
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ kafka_to_glue_dag.py
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer.py
â”‚   â””â”€â”€ consumer_api.py
â”œâ”€â”€ glue/
â”‚   â””â”€â”€ glue_job.py
â”œâ”€â”€ snowflake/
â”‚   â”œâ”€â”€ storage_integration.sql
â”‚   â”œâ”€â”€ stage_curated.sql
â”‚   â”œâ”€â”€ stage_rejected.sql
â”‚   â”œâ”€â”€ pipe_curated.sql
â”‚   â””â”€â”€ pipe_rejected.sql
â”œâ”€â”€ s3/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ curated/
â”‚   â””â”€â”€ rejected/
â”œâ”€â”€ architecture_diagram.png
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
ğŸ”§ Technologies Used
Layer	Tech
Real-time ingestion	Kafka (EC2)
API Trigger	Python + Flask (EC2)
Orchestration	Apache Airflow
ETL	AWS Glue + PySpark
Data Quality	Great Expectations
Data Format	Delta Lake
Storage	Amazon S3
Warehouse	Snowflake + Snowpipe
ğŸ§ª Data Quality Rules (Great Expectations)

NOT NULL checks: truck_id, timestamp, latitude, longitude

Type Validation:

latitude & longitude â†’ float

speed â†’ float or integer

Value Range Validation:

latitude between -90 to 90

longitude between -180 to 180

speed â‰¥ 0

JSON Structure Validation:

Fields must match schema: truck_id, location.latitude, location.longitude, speed, timestamp

Records failing these rules â†’ rejected folder.

ğŸª„ Airflow Dynamic Orchestration

DAG calls consumer API: http://ec2-public-ip/start-consumer

Consumer listens to Kafka.

Once a message is received, consumer responds to Airflow.

Airflow waits 5 minutes for more data â†’ triggers Glue job.

Avoids schedule-based waste; pipeline is fully event-driven.

ğŸ§° Glue Job Workflow

Read raw JSON from S3.

Apply Great Expectations checks.

Split into:

curated_df â†’ valid

rejected_df â†’ invalid

Write both to S3 in Delta format.

Snowpipe automatically ingests them into Snowflake.

â„ï¸ Snowflake Setup

Inside snowflake/ folder:

Storage integration

External stages

File formats

Snowpipes for curated & rejected data

Snowpipe continuously loads new Delta files into Snowflake tables.

â–¶ï¸ How to Run

Start Kafka Producer

python kafka/producer.py

Start Kafka Consumer API

python kafka/consumer_api.py

Trigger Airflow DAG
Airflow UI â†’ Trigger DAG â†’ waits for data â†’ runs Glue job.

Check S3 Outputs

s3://bucket/curated/
s3://bucket/rejected/

Check Snowflake Tables

SELECT * FROM LOGISTICS_CURATED_TABLE;
SELECT * FROM LOGISTICS_REJECTED_TABLE;
ğŸ“¸ Architecture Diagram

Refer to architecture_diagram.png

ğŸ‘¤ Author

K Syed Khalid Hameed
